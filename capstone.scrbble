#lang scribble/book

@(require scribble-code-examples)
@(require "util.rkt")


@title{Programming Language Implementation}

@(table-of-contents)

@section{Background}

TODO: quick racket guide

TODO: regular expressions

@section{Parsing and Semantic Analysis}

TODO: give analogy

The first step taken by any interpretor or compiler is parsing. The
parsing step converts the plain-text source code into IR (intermediate
representation) - usually a tree of some sort. The tree can then be passed
onto another compilation pass to do semantic analysis, interpretation,
code generation or some other transformation.

Of course the parser needs a valid source file to be able to convert into
a tree, so in translating the source to an IR, it checks the syntax of the
code. If the syntax is invalid, it can emit a syntax error, and either
fail or attempt to continue parsing the rest of the file (if it's
possible).

There are a few ways to implement a parser, each with its own advantages
and disadvantages. The methods covered in this paper are:

@itemlist[
@item{A @tt{lex}/@tt{yacc} parser}
@item{A packrat parser}
@item{A parser combinator}
]

@subsection{Formal languages}

@subsection{Lexing}

Lexing is a step that "tokenizes" the input source code. The tokenisation
process just splits the source up into a set of words, each with an
associated type (i.e. keyword, number, identifier, etc.). One of the
advantages of lexing is it removes non-semantic whitespace from the code,
so any spurious spaces, tabs or newlines are removed. This can simplify
the parser and make the logic in the later compiler passes simpler.

For instance, consider a string such as:

"The quick brown fox jumps over the lazy dog"

We could use spaces as a delimiter for words, and we could assign the
nouns that are animals with a special `ANIMAL` tag.

@code-examples[#:lang "at-exp racket" #:context #'here]|{
    (define string-to-lex "The quick brown fox jumps over the lazy dog")
    (define animals '("dog" "cat" "fox"))
    (define (animal? a)
     (ormap
      (lambda (q) (equal? q a))
            animals))
    (map (lambda (s)
          (match s
           ((? animal? s) `(ANIMAL ,s))
           ((? string? s) `(TOKEN ,s))))
     (string-split string-to-lex))
}|

This token list could then be passed on the parser to build the parse tree.

In Racket (and other languages that support pattern matching), lexing is
very easy to do without any library support, but since @tt{lex} (and tools
like it) are popular in other languages, it's worth looking at how to
write a lexer using @tt{lex}.

Typically, @tt{lex} uses regular expressions to match the incoming text to
make pattern matching simpler.

A simple, C-like grammer could have have a @tt{lex} file like:

@code{
NUMBER  [0-9]*
ID      [a-z][a-z0-9_]*
IF      if
FOR     for
DO      do
WHILE   while
TIMES   *
DIVIDE  /
PLUS    +
MINUS   -
POUND   #
LBRACE  {
RBRACE  }
LPAR    (
RPAR    )
LSQUARE [
RSQUARE ]
EQUAL   =
PEQUAL  +=
MEQUAL  -=
}

After embedding the lexer code in a specially annotated block in
a C program, all that has to be done to generate the lexer is to call
@tt{lex} on the file and compile the resulting C file.

Packrat parsers don't utilize lexers (since they rely on matching strings
with the parser), nor do parser combinators, usually (though it is
possible). Nevertheless, @tt{lex} and @tt{yacc} are a common combo for
building high-quality, fast parsers.

@section{Interpreters}

@subsection{Graph walking}

@subsection{Virtual Machine}

@subsubsection{Stack based}

@subsubsection{Register based}

@subsubsection{JIT compilation}

@section{Compilers}

@subsection{Optimization passes}

@subsection{Code generation}
